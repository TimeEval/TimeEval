{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TSB-UAD: An End-to-End Benchmark Suite for Univariate Time-Series Anomaly Detection\n",
    "\n",
    "- Source and description: https://github.com/TheDatumOrg/TSB-UAD\n",
    "- Paper: https://dl.acm.org/doi/pdf/10.14778/3529337.3529354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from config import data_raw_folder, data_processed_folder\n",
    "from timeeval import DatasetManager, Datasets\n",
    "from timeeval.datasets import DatasetAnalyzer, DatasetRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_datasets(folder):\n",
    "    if not isinstance(folder, Path):\n",
    "        folder = Path(folder)\n",
    "    return sorted([f for f in folder.glob(\"*.out\") if f.is_file()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_collection_name = \"TSB-UAD\"\n",
    "source_folder = Path(data_raw_folder) / \"TSB-UAD\"\n",
    "target_folder = Path(data_processed_folder)\n",
    "\n",
    "print(f\"Looking for source datasets in {Path(source_folder).absolute()} and\\nsaving processed datasets in {Path(target_folder).absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# shared by all datasets\n",
    "dataset_type = \"synthetic\"\n",
    "input_type = \"univariate\"\n",
    "datetime_index = False\n",
    "split_at = None\n",
    "train_is_normal = False\n",
    "train_type = \"unsupervised\"\n",
    "\n",
    "dm = DatasetManager(target_folder, create_if_missing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_dataset(dm: DatasetManager, idx: int, f: Path, name_prefix: str = \"\") -> None:\n",
    "    print(f\"> Processing source dataset {idx}\")\n",
    "    dataset_name = f\"{name_prefix}-{f.stem}\"\n",
    "    test_filename = f\"{dataset_name}.test.csv\"\n",
    "    test_path = dataset_subfolder / test_filename\n",
    "    target_test_filepath = target_subfolder / test_filename\n",
    "    target_meta_filepath = target_test_filepath.parent / f\"{dataset_name}.{Datasets.METADATA_FILENAME_SUFFIX}\"\n",
    "\n",
    "    # Prepare datasets\n",
    "    if not target_test_filepath.exists() or not target_meta_filepath.exists():\n",
    "        df_test = pd.read_csv(f, header=None)\n",
    "        df_test.columns = [\"value\", \"is_anomaly\"]\n",
    "        df_test.insert(0, \"timestamp\", df_test.index)\n",
    "        df_test.to_csv(target_test_filepath, index=False)\n",
    "        print(f\"  written dataset {idx}\")\n",
    "    else:\n",
    "        df_test = None\n",
    "        print(f\"  skipped writing dataset {idx} to disk, because it already exists.\")\n",
    "\n",
    "    # Prepare metadata\n",
    "    def analyze(df_test):\n",
    "        da = DatasetAnalyzer((dataset_collection_name, dataset_name), is_train=False, df=df_test, ignore_stationarity=True)\n",
    "        da.save_to_json(target_meta_filepath, overwrite=True)\n",
    "        meta = da.metadata\n",
    "        print(f\"  analyzed test dataset {idx}\")\n",
    "        return meta\n",
    "\n",
    "    if target_meta_filepath.exists():\n",
    "        try:\n",
    "            meta = DatasetAnalyzer.load_from_json(target_meta_filepath, train=False)\n",
    "            print(f\"  skipped analyzing dataset {idx}, because metadata already exists.\")\n",
    "        except ValueError:\n",
    "            if df_test is None:\n",
    "                df_test = pd.read_csv(target_test_filepath)\n",
    "            meta = analyze(df_test)\n",
    "    else:\n",
    "        meta = analyze(df_test)\n",
    "\n",
    "    dm.add_dataset(DatasetRecord(\n",
    "          collection_name=dataset_collection_name,\n",
    "          dataset_name=dataset_name,\n",
    "          train_path=None,\n",
    "          test_path=test_path,\n",
    "          dataset_type=dataset_type,\n",
    "          datetime_index=datetime_index,\n",
    "          split_at=split_at,\n",
    "          train_type=train_type,\n",
    "          train_is_normal=train_is_normal,\n",
    "          input_type=input_type,\n",
    "          length=meta.length,\n",
    "          dimensions=meta.dimensions,\n",
    "          contamination=meta.contamination,\n",
    "          num_anomalies=meta.num_anomalies,\n",
    "          min_anomaly_length=meta.anomaly_length.min,\n",
    "          median_anomaly_length=meta.anomaly_length.median,\n",
    "          max_anomaly_length=meta.anomaly_length.max,\n",
    "          mean=meta.mean,\n",
    "          stddev=meta.stddev,\n",
    "          trend=meta.trend,\n",
    "          stationarity=meta.get_stationarity_name(),\n",
    "          period_size=np.nan\n",
    "    ))\n",
    "    print(f\"... processed source dataset {idx}: {name_prefix}-{f.name} -> {target_test_filepath}\")\n",
    "\n",
    "subcollection = \"artificial\"\n",
    "\n",
    "print(\"#############\")\n",
    "print(f\"# Processing sub-collection {subcollection}\")\n",
    "print(\"#############\")\n",
    "\n",
    "# create target directory\n",
    "dataset_subfolder = Path(input_type) / f\"{dataset_collection_name}-{subcollection}\"\n",
    "target_subfolder = target_folder / dataset_subfolder\n",
    "target_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created directories {target_subfolder}\")\n",
    "\n",
    "folder = source_folder / subcollection\n",
    "\n",
    "i = 0\n",
    "for file in find_datasets(folder):\n",
    "    process_dataset(dm, i, file)\n",
    "    i += 1\n",
    "dm.save()\n",
    "\n",
    "\n",
    "subcollection = \"synthetic\"\n",
    "\n",
    "print(\"#############\")\n",
    "print(f\"# Processing sub-collection {subcollection}\")\n",
    "print(\"#############\")\n",
    "\n",
    "# create target directory\n",
    "dataset_subfolder = Path(input_type) / f\"{dataset_collection_name}-{subcollection}\"\n",
    "target_subfolder = target_folder / dataset_subfolder\n",
    "target_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created directories {target_subfolder}\")\n",
    "\n",
    "folder = source_folder / subcollection\n",
    "\n",
    "for subfolder in folder.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        for file in find_datasets(subfolder):\n",
    "            process_dataset(dm, i, file, name_prefix=subfolder.stem)\n",
    "            i += 1\n",
    "dm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dm.refresh()\n",
    "dm.df().loc[(slice(dataset_collection_name,dataset_collection_name), slice(None))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = source_folder / \"artificial\"\n",
    "datasets = find_datasets(folder)\n",
    "[d.name for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = datasets[10]\n",
    "df = pd.read_csv(f, header=None)\n",
    "df.columns = [\"value\", \"is_anomaly\"]\n",
    "df.insert(0, \"timestamp\", df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.plot(subplots=True)\n",
    "# plt.xlim(anomaly[0]-1500, anomaly[1]+1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}