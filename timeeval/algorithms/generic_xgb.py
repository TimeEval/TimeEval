# DO NOT EDIT THIS FILE!
# This file was automatically generated using the timeeval_experiments.generator from the template:
# timeeval_experiments/generator/templates/docker-algorithm.py.jinja
from durations import Duration
from typing import Any, Dict, Optional

from timeeval import Algorithm, TrainingType, InputDimensionality
from timeeval.adapters import DockerAdapter
from timeeval.params import ParameterConfig


_generic_xgb_parameters: Dict[str, Dict[str, Any]] = {
 "booster": {
  "defaultValue": "gbtree",
  "description": "Booster to use",
  "name": "booster",
  "type": "enum[gbtree,gblinear,dart]"
 },
 "colsample_bylevel": {
  "defaultValue": None,
  "description": "Subsample ratio of columns for each level.",
  "name": "colsample_bylevel",
  "type": "float"
 },
 "colsample_bynode": {
  "defaultValue": None,
  "description": "Subsample ratio of columns for each split.",
  "name": "colsample_bynode",
  "type": "float"
 },
 "colsample_bytree": {
  "defaultValue": None,
  "description": "Subsample ratio of columns when constructing each tree.",
  "name": "colsample_bytree",
  "type": "float"
 },
 "learning_rate": {
  "defaultValue": 0.1,
  "description": "Boosting learning rate (xgb\u2019s `eta`)",
  "name": "learning_rate",
  "type": "float"
 },
 "max_depth": {
  "defaultValue": None,
  "description": "Maximum tree depth for base learners.",
  "name": "max_depth",
  "type": "int"
 },
 "max_samples": {
  "defaultValue": None,
  "description": "Subsample ratio of the training instance.",
  "name": "max_samples",
  "type": "float"
 },
 "n_estimators": {
  "defaultValue": 100,
  "description": "Number of gradient boosted trees. Equivalent to number of boosting rounds.",
  "name": "n_estimators",
  "type": "int"
 },
 "n_jobs": {
  "defaultValue": 1,
  "description": "The number of jobs to run in parallel. `-1` means using all processors.",
  "name": "n_jobs",
  "type": "int"
 },
 "n_trees": {
  "defaultValue": 1,
  "description": "If >1, then boosting random forests with `n_trees` trees.",
  "name": "n_trees",
  "type": "int"
 },
 "random_state": {
  "defaultValue": 42,
  "description": "Seeds the randomness of the bootstrapping and the sampling of the features.",
  "name": "random_state",
  "type": "int"
 },
 "train_window_size": {
  "defaultValue": 50,
  "description": "Size of the training windows. Always predicts a single point!",
  "name": "train_window_size",
  "type": "int"
 },
 "tree_method": {
  "defaultValue": "auto",
  "description": "Tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. `exact` is slowest, `hist` is fastest. Prefer `hist` and `approx` over `exact`, because for most datasets they have comparative quality, but are significantly faster.",
  "name": "tree_method",
  "type": "enum[auto,exact,approx,hist]"
 },
 "verbose": {
  "defaultValue": 0,
  "description": "Controls logging verbosity.",
  "name": "verbose",
  "type": "int"
 }
}


def generic_xgb(params: Optional[ParameterConfig] = None, skip_pull: bool = False, timeout: Optional[Duration] = None) -> Algorithm:
    """XGBoosting (RR)

    A generic windowed forecasting method using XGBoost regression (requested by RollsRoyce). The forecasting error is used as anomaly score.


    **Algorithm Parameters:**

    train_window_size: int
        Size of the training windows. Always predicts a single point! (default: ``50``)
    n_estimators: int
        Number of gradient boosted trees. Equivalent to number of boosting rounds. (default: ``100``)
    learning_rate: float
        Boosting learning rate (xgbâ€™s `eta`) (default: ``0.1``)
    booster: enum[gbtree,gblinear,dart]
        Booster to use (default: ``gbtree``)
    tree_method: enum[auto,exact,approx,hist]
        Tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. `exact` is slowest, `hist` is fastest. Prefer `hist` and `approx` over `exact`, because for most datasets they have comparative quality, but are significantly faster. (default: ``auto``)
    n_trees: int
        If >1, then boosting random forests with `n_trees` trees. (default: ``1``)
    max_depth: int
        Maximum tree depth for base learners. (default: ``None``)
    max_samples: float
        Subsample ratio of the training instance. (default: ``None``)
    colsample_bytree: float
        Subsample ratio of columns when constructing each tree. (default: ``None``)
    colsample_bylevel: float
        Subsample ratio of columns for each level. (default: ``None``)
    colsample_bynode: float
        Subsample ratio of columns for each split. (default: ``None``)
    random_state: int
        Seeds the randomness of the bootstrapping and the sampling of the features. (default: ``42``)
    verbose: int
        Controls logging verbosity. (default: ``0``)
    n_jobs: int
        The number of jobs to run in parallel. `-1` means using all processors. (default: ``1``)

    Parameters
    ----------
    params : Optional[ParameterConfig]
        Parameter configuration for the algorithm
    skip_pull : bool
        Set to ``True`` to skip pulling the Docker image and use a local image instead.
        If the image is not present locally, this will raise an error.
    timeout : Optional[Duration]
        Set an individual execution and training timeout for this algorithm.
        This will overwrite the global timeouts set using :class:`~timeeval.ResourceConstraints`.

    Returns
    -------
    ~timeeval.Algorithm
        A correctly configured :class:`~timeeval.Algorithm` object for the XGBoosting (RR) algorithm.
    """
    return Algorithm(
        name="XGBoosting (RR)",
        main=DockerAdapter(
            image_name="ghcr.io/timeeval/generic_xgb",
            tag="0.3.0",
            skip_pull=skip_pull,
            timeout=timeout,
            group_privileges="akita",
        ),
        preprocess=None,
        postprocess=None,
        param_schema=_generic_xgb_parameters,
        param_config=params or ParameterConfig.defaults(),
        data_as_file=True,
        training_type=TrainingType.SEMI_SUPERVISED,
        input_dimensionality=InputDimensionality("univariate")
    )
